{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Python version: 3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)]\n",
      "pandas version: 1.3.2\n",
      "matplotlib version: 3.4.3\n",
      "NumPy version: 1.22.4\n",
      "scikit-learn version: 0.24.2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Sytem\n",
    "import sys,os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import IPython\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Visualidation\n",
    "import sweetviz as sv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "#EDA\n",
    "import sweetviz as sv\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "#model\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV,StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('-'*40)\n",
    "\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = \"Perished\"\n",
    "PATH_LOG = \"../logs/log.csv\"\n",
    "ID = 12\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Shape of Train Data\n",
      "[row :891,column :22]\n",
      "-------------------------\n",
      "Shape of Test Data\n",
      "[row :418,column :22]\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/input/train_beta.csv')\n",
    "df_test = pd.read_csv('../data/input/test_beta.csv')\n",
    "sample = pd.read_csv('../data/input/gender_submission.csv')\n",
    "data = pd.concat([df_train,df_test],sort=False)\n",
    "print('-'*25)\n",
    "print(\"Shape of Train Data\\n[row :{},column :{}]\".format(\n",
    "    df_train.shape[0], df_train.shape[1]))\n",
    "print('-'*25)\n",
    "print(\"Shape of Test Data\\n[row :{},column :{}]\".format(\n",
    "    df_test.shape[0], df_test.shape[1]))\n",
    "print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns =  [\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",\"Title\",'FareBin',\"Fare_Code\",\"Title_Code\",\"Family\",\"Embarked_Code\"]\n",
    "dataset = data.drop(columns=drop_columns)\n",
    "train = dataset[:len(df_train)]\n",
    "test = dataset[len(df_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "各1層目モデルに対する操作（定義・訓練・予測）を簡略化するため、分類器の拡張クラスを定義します。\n",
    "\"\"\"\n",
    "class ClfBuilder(object):\n",
    "    def __init__(self, clf, params=None):\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\"\"\"\n",
    "スタッキングでは2層目のモデルに1層目のモデルの予測値を利用します。2層目で既知のデータに対する過学習を防ぐため、1層目でOut-of-Foldによる予測値を算出してこれを2層目の学習に利用します。以下の実装では、StratifiedKFoldで5分割の交差検証を行っています。\n",
    "\"\"\"\n",
    "def get_base_model_preds(clf, X_train, y_train, X_test):\n",
    "    print(clf.clf)\n",
    "\n",
    "    N_SPLITS = 5\n",
    "    oof_valid = np.zeros((X_train.shape[0], NUM_CLASSES))\n",
    "    oof_test = np.zeros((X_test.shape[0], NUM_CLASSES))\n",
    "    oof_test_skf = np.zeros((N_SPLITS, X_test.shape[0], NUM_CLASSES))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS)\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        print('[CV] {}/{}'.format(i+1, N_SPLITS))\n",
    "        X_train_, X_valid_ = X_train[train_index], X_train[valid_index]\n",
    "        y_train_, y_valid_ = y_train[train_index], y_train[valid_index]\n",
    "\n",
    "        clf.fit(X_train_, y_train_)\n",
    "\n",
    "        oof_valid[valid_index] = clf.predict_proba(X_valid_)\n",
    "        oof_test_skf[i, :] = clf.predict_proba(X_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_valid, oof_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1_params = {\n",
    "\n",
    "}\n",
    "rfc1_1_params = {\n",
    "    'bootstrap': True,\n",
    "    'max_depth': 110,\n",
    "    'max_features':'auto', \n",
    "    'min_samples_leaf':1,\n",
    "    'min_samples_split':10, \n",
    "    'n_estimators':200,\n",
    "    'random_state':SEED\n",
    "\n",
    "}\n",
    "rfc1_2_params = {\n",
    "    'bootstrap': True,\n",
    "    'max_depth': 10,\n",
    "    'max_features':'sqrt', \n",
    "    'min_samples_leaf':1,\n",
    "    'min_samples_split':5, \n",
    "    'n_estimators':1600,\n",
    "    'random_state':SEED\n",
    "\n",
    "}\n",
    "gbc1_params = {\n",
    "    \"learning_rate\":0.01,\n",
    "    \"max_depth\":50,\n",
    "    \"max_features\":'sqrt', \n",
    "    \"min_samples_leaf\":4,\n",
    "    \"min_samples_split\":10,\n",
    "    \"n_estimators\":200\n",
    "\n",
    "}\n",
    "etc1_params = {\n",
    "\n",
    "}\n",
    "xgbc1_1_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 10,\n",
    "    'random_state': 0, \n",
    "}\n",
    "xgbc1_2_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 100,\n",
    "    'random_state': 0, \n",
    "    \n",
    "}\n",
    "xgbc1_3_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 200,\n",
    "    'random_state': 0, \n",
    "    \n",
    "}\n",
    "#NN\n",
    "mlp1_1_params = {\n",
    "    'hidden_layer_sizes':12\n",
    "}\n",
    "mlp1_2_params = {\n",
    "    'hidden_layer_sizes':52\n",
    "\n",
    "}\n",
    "mlp1_3_params = {\n",
    "    'hidden_layer_sizes':102\n",
    "\n",
    "}\n",
    "knn1_1_params = {\n",
    "    'n_neighbors': 4\n",
    "\n",
    "}\n",
    "knn1_2_params = {\n",
    "    'n_neighbors': 16\n",
    "    \n",
    "}\n",
    "knn1_3_params = {\n",
    "    'n_neighbors': 32\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "lr1 = ClfBuilder(clf=LogisticRegression,params=lr1_params)\n",
    "#RF\n",
    "rfc1_1 = ClfBuilder(clf=RandomForestClassifier, params=rfc1_1_params)\n",
    "rfc1_2 = ClfBuilder(clf=RandomForestClassifier, params=rfc1_2_params)\n",
    "#Tree\n",
    "gbc1 = ClfBuilder(clf=GradientBoostingClassifier, params=gbc1_params)\n",
    "etc1 = ClfBuilder(clf=ExtraTreesClassifier, params=etc1_params)\n",
    "xgbc1_1 = ClfBuilder(clf=XGBClassifier, params=xgbc1_1_params)\n",
    "xgbc1_2 = ClfBuilder(clf=XGBClassifier, params=xgbc1_2_params)\n",
    "xgbc1_3 = ClfBuilder(clf=XGBClassifier, params=xgbc1_3_params)\n",
    "#NN\n",
    "mlp1_1 = ClfBuilder(clf=MLPClassifier,params=mlp1_1_params)\n",
    "mlp1_2 = ClfBuilder(clf=MLPClassifier,params=mlp1_2_params)\n",
    "mlp1_3 = ClfBuilder(clf=MLPClassifier,params=mlp1_3_params)\n",
    "#Other\n",
    "knn1_1 = ClfBuilder(clf=KNeighborsClassifier,params=knn1_1_params)\n",
    "knn1_2 = ClfBuilder(clf=KNeighborsClassifier,params=knn1_2_params)\n",
    "knn1_3 = ClfBuilder(clf=KNeighborsClassifier,params=knn1_3_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, 2:].values\n",
    "y_train = train.iloc[:, 1].values\n",
    "X_test = test.iloc[:, 2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "RandomForestClassifier(max_depth=110, min_samples_split=10, n_estimators=200,\n",
      "                       random_state=42)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "RandomForestClassifier(max_depth=10, max_features='sqrt', min_samples_split=5,\n",
      "                       n_estimators=1600, random_state=42)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "GradientBoostingClassifier(learning_rate=0.01, max_depth=50,\n",
      "                           max_features='sqrt', min_samples_leaf=4,\n",
      "                           min_samples_split=10, n_estimators=200)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "ExtraTreesClassifier()\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=100,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=200,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "MLPClassifier(hidden_layer_sizes=12)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "MLPClassifier(hidden_layer_sizes=52)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "MLPClassifier(hidden_layer_sizes=102)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "KNeighborsClassifier(n_neighbors=4)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "KNeighborsClassifier(n_neighbors=16)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "KNeighborsClassifier(n_neighbors=32)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n"
     ]
    }
   ],
   "source": [
    "oof_valid_lr1,oof_test_lr1 = get_base_model_preds(lr1,X_train,y_train,X_test)\n",
    "oof_valid_rfc1_1,oof_test_rfc1_1 = get_base_model_preds(rfc1_1,X_train,y_train,X_test)\n",
    "oof_valid_rfc1_2,oof_test_rfc1_2 = get_base_model_preds(rfc1_2,X_train,y_train,X_test)\n",
    "oof_valid_gbc1,oof_test_gbc1 = get_base_model_preds(gbc1,X_train,y_train,X_test)\n",
    "oof_valid_etc1,oof_test_etc1 = get_base_model_preds(etc1,X_train,y_train,X_test)\n",
    "oof_valid_xgbc1_1,oof_test_xgbc1_1 = get_base_model_preds(xgbc1_1,X_train,y_train,X_test)\n",
    "oof_valid_xgbc1_2,oof_test_xgbc1_2 = get_base_model_preds(xgbc1_2,X_train,y_train,X_test)\n",
    "oof_valid_xgbc1_3,oof_test_xgbc1_3 = get_base_model_preds(xgbc1_3,X_train,y_train,X_test)\n",
    "oof_valid_mlp1_1,oof_test_mlp1_1 = get_base_model_preds(mlp1_1,X_train,y_train,X_test)\n",
    "oof_valid_mlp1_2,oof_test_mlp1_2 = get_base_model_preds(mlp1_2,X_train,y_train,X_test)\n",
    "oof_valid_mlp1_3,oof_test_mlp1_3 = get_base_model_preds(mlp1_3,X_train,y_train,X_test)\n",
    "oof_valid_knn1_1,oof_test_knn1_1 = get_base_model_preds(knn1_1,X_train,y_train,X_test)\n",
    "oof_valid_knn1_2,oof_test_knn1_2 = get_base_model_preds(knn1_2,X_train,y_train,X_test)\n",
    "oof_valid_knn1_3,oof_test_knn1_3 = get_base_model_preds(knn1_3,X_train,y_train,X_test)\n",
    "# oof_valid_,oof_test_ = get_base_model_preds(,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_base = np.concatenate([oof_valid_lr1,\n",
    "oof_valid_rfc1_1,\n",
    "oof_valid_rfc1_2,\n",
    "oof_valid_gbc1,\n",
    "oof_valid_etc1,\n",
    "oof_valid_xgbc1_1,\n",
    "oof_valid_xgbc1_2,\n",
    "oof_valid_xgbc1_3,\n",
    "oof_valid_mlp1_1,\n",
    "oof_valid_mlp1_2,\n",
    "oof_valid_mlp1_3,\n",
    "oof_valid_knn1_1,\n",
    "oof_valid_knn1_2,\n",
    "oof_valid_knn1_3\n",
    "                              ], axis=1)\n",
    "X_test_base = np.concatenate([\n",
    "    oof_test_lr1,\n",
    "    oof_test_rfc1_1,\n",
    "    oof_test_rfc1_2,\n",
    "    oof_test_gbc1,\n",
    "    oof_test_etc1,\n",
    "    oof_test_xgbc1_1,\n",
    "    oof_test_xgbc1_2,\n",
    "    oof_test_xgbc1_3,\n",
    "    oof_test_mlp1_1,\n",
    "    oof_test_mlp1_2,\n",
    "    oof_test_mlp1_3,\n",
    "    oof_test_knn1_1,\n",
    "    oof_test_knn1_2,\n",
    "    oof_test_knn1_3\n",
    "                             ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc2_1_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 10,\n",
    "    'random_state': SEED, \n",
    "}\n",
    "xgbc2_2_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 100,\n",
    "    'random_state': SEED, \n",
    "    \n",
    "}\n",
    "xgbc2_3_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 200,\n",
    "    'random_state': SEED, \n",
    "    \n",
    "}\n",
    "rfc2_1_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 10,\n",
    "    'random_state': SEED, \n",
    "}\n",
    "rfc2_2_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 100,\n",
    "    'random_state': SEED, \n",
    "    \n",
    "}\n",
    "rfc2_3_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 200,\n",
    "    'random_state': SEED, \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc2_1 = ClfBuilder(clf=XGBClassifier,params= xgbc2_1_params) \n",
    "xgbc2_2 = ClfBuilder(clf=XGBClassifier,params= xgbc2_2_params)\n",
    "xgbc2_3 = ClfBuilder(clf=XGBClassifier,params=xgbc2_3_params)\n",
    "rfc2_1 = ClfBuilder(clf=RandomForestClassifier,params= rfc2_1_params) \n",
    "rfc2_2 = ClfBuilder(clf=RandomForestClassifier,params= rfc2_2_params)\n",
    "rfc2_3 = ClfBuilder(clf=RandomForestClassifier,params=rfc2_3_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=42,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=100,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=42,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=200,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=42,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "RandomForestClassifier(max_depth=10, n_estimators=500, random_state=42)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "RandomForestClassifier(max_depth=100, n_estimators=500, random_state=42)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n",
      "RandomForestClassifier(max_depth=200, n_estimators=500, random_state=42)\n",
      "[CV] 1/5\n",
      "[CV] 2/5\n",
      "[CV] 3/5\n",
      "[CV] 4/5\n",
      "[CV] 5/5\n"
     ]
    }
   ],
   "source": [
    "oof_valid_2_1,oof_test_2_1 = get_base_model_preds(xgbc2_1,X_train_base,y_train,X_test_base)\n",
    "oof_valid_2_2,oof_test_2_2 = get_base_model_preds(xgbc2_2,X_train_base,y_train,X_test_base)\n",
    "oof_valid_2_3,oof_test_2_3 = get_base_model_preds(xgbc2_3,X_train_base,y_train,X_test_base)\n",
    "oof_valid_r_1,oof_test_r_1 = get_base_model_preds(rfc2_1,X_train_base,y_train,X_test_base)\n",
    "oof_valid_r_2,oof_test_r_2 = get_base_model_preds(rfc2_2,X_train_base,y_train,X_test_base)\n",
    "oof_valid_r_3,oof_test_r_3 = get_base_model_preds(rfc2_3,X_train_base,y_train,X_test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_base_2 = np.concatenate([\n",
    "    oof_valid_2_1,\n",
    "    oof_valid_2_2,\n",
    "    oof_valid_2_3,\n",
    "    oof_valid_r_1,\n",
    "    oof_valid_r_2,\n",
    "    oof_valid_r_3\n",
    "    ],axis=1\n",
    ")\n",
    "X_test_base_2 = np.concatenate([\n",
    "    oof_test_2_1,\n",
    "    oof_test_2_2,\n",
    "    oof_test_2_3,\n",
    "    oof_test_r_1,\n",
    "    oof_test_r_2,\n",
    "    oof_test_r_3\n",
    "],axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=110, min_samples_split=10, n_estimators=200,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model1 =XGBClassifier()\n",
    "final_model2 = RandomForestClassifier(random_state=SEED)\n",
    "final_model3 = MLPClassifier()\n",
    "final_model1.fit(X_train_base_2,y_train)\n",
    "final_model2.fit(X_train_base_2,y_train)\n",
    "final_model3.fit(X_train_base_2,y_train)\n",
    "basemodel= RandomForestClassifier(bootstrap=True,max_depth=110,max_features='auto', min_samples_leaf=1,min_samples_split=10,n_estimators=200,random_state=SEED)\n",
    "basemodel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = final_model1.predict_proba(X_test_base_2)\n",
    "pred2 = final_model2.predict_proba(X_test_base_2)\n",
    "pred3 = final_model3.predict_proba(X_test_base_2)\n",
    "bpred = basemodel.predict_proba(X_test)\n",
    "predict = ((6*(pred1*0.5+pred2*0.3+pred3*0.2)+4*bpred)).argmax(axis=1)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9887766554433222\n"
     ]
    }
   ],
   "source": [
    "STrain = final_model1.score(X_train_base_2,y_train)\n",
    "Search = \"x\"\n",
    "CV = \"x\"\n",
    "bestparam = \"x\"\n",
    "\n",
    "print('Train Score: {}'.format(STrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(PATH_LOG)\n",
    "Row  =  len(log)\n",
    "log.loc[Row] = \"x\"\n",
    "log.loc[Row,\"Modelname\"] = \"Ensemble\"\n",
    "log.loc[Row,\"Search\"] = Search\n",
    "log.loc[Row,\"Vestparams\"] = bestparam\n",
    "log.loc[Row,\"CV\"] = CV\n",
    "log.loc[Row,\"STrain\"] = STrain\n",
    "log.loc[Row,\"SValid\"] = SValid\n",
    "log.loc[Row,\"Age_Code_N\"] = 3\n",
    "use_columns = test.columns.drop([\"PassengerId\",\"Perished\"])\n",
    "for col in use_columns:\n",
    "    log.loc[Row,col] = \"o\"\n",
    "log.fillna(\"x\",inplace=True)\n",
    "# if PublicScore is knowed\n",
    "#log.loc[Row,\"SPublic\"] = 0.791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelname</th>\n",
       "      <th>STrain</th>\n",
       "      <th>SValid</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>C</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_Code</th>\n",
       "      <th>Embarked_Code</th>\n",
       "      <th>Search</th>\n",
       "      <th>CV</th>\n",
       "      <th>Bestparams</th>\n",
       "      <th>Title</th>\n",
       "      <th>FareBin</th>\n",
       "      <th>Fare_Code</th>\n",
       "      <th>Vestparams</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.8918539325842697</td>\n",
       "      <td>0.8156424581005587</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0.8058361391694725</td>\n",
       "      <td>0.8058361391694725</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0.9932659932659933</td>\n",
       "      <td>0.9932659932659933</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0.9438832772166106</td>\n",
       "      <td>0.9438832772166106</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0.988777</td>\n",
       "      <td>0.826038</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>...</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Modelname              STrain              SValid Pclass Sex Age SibSp  \\\n",
       "60       XGB  0.8918539325842697  0.8156424581005587      o   o   o     o   \n",
       "61  Ensemble  0.8058361391694725  0.8058361391694725      o   o   o     o   \n",
       "62  Ensemble  0.9932659932659933  0.9932659932659933      o   o   o     o   \n",
       "63  Ensemble  0.9438832772166106  0.9438832772166106      o   o   o     o   \n",
       "64  Ensemble            0.988777            0.826038      o   o   o     o   \n",
       "\n",
       "   Parch Fare  C  ... Title_Code Embarked_Code Search CV Bestparams Title  \\\n",
       "60     o    o  o  ...          x             x      x  x          x     x   \n",
       "61     o    o  o  ...          x             x      x  x          x     x   \n",
       "62     o    o  o  ...          x             x      x  x          x     x   \n",
       "63     o    o  o  ...          x             x      x  x          x     x   \n",
       "64     o    o  o  ...          x             x      x  x          x     x   \n",
       "\n",
       "   FareBin Fare_Code Vestparams Family  \n",
       "60       x         x          x      x  \n",
       "61       x         x          x      x  \n",
       "62       x         x          x      x  \n",
       "63       x         x          x      x  \n",
       "64       x         x          x      x  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.to_csv(PATH_LOG,index_label=False)\n",
    "log.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/output/12_submission_ensemble3.csv\n",
      "Submission Format:(418, 2) \n",
      "My Sumbisison Format:(418, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Perished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Perished\n",
       "0          892         1\n",
       "1          893         1\n",
       "2          894         1\n",
       "3          895         1\n",
       "4          896         1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_sub =  \"../data/output/\"+str(ID)+\"_submission_\"+\"ensemble3\"+\".csv\"\n",
    "print(PATH_sub)\n",
    "submission_df = pd.DataFrame(columns=['PassengerId', Target])\n",
    "submission_df['PassengerId'] = df_test['PassengerId']\n",
    "submission_df[Target] = predict\n",
    "submission_df.reset_index(drop=True, inplace=True)\n",
    "submission_df.to_csv(PATH_sub, header=True, index=False)\n",
    "print(\"Submission Format:{} \".format(sample.shape))\n",
    "print(\"My Sumbisison Format:{}\".format(submission_df.shape))\n",
    "submission_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68ea084522a32e1c5efebe8e26724b4feaefc93bc2807a5a214dcc5b0260bfd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
